{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Class Model for Multivariate Categorical Data\n",
    "\n",
    "Implementation of the latent class model with EM algorithm as described in the notes.\n",
    "\n",
    "## Model\n",
    "\n",
    "Let $X = (X^{(1)}, \\ldots, X^{(m)})$ be a vector of categorical variables, where $X^{(r)} \\in \\{1, 2, \\ldots, C_r\\}$.\n",
    "\n",
    "We introduce a latent class variable $H \\in \\{1, \\ldots, K\\}$ and assume conditional independence:\n",
    "\n",
    "$$P(X = x) = \\sum_{k=1}^K P(H = k) \\prod_{r=1}^m P(X^{(r)} = x^{(r)} | H = k)$$\n",
    "\n",
    "## Parameters\n",
    "\n",
    "- Mixture weights: $\\pi_k = P(H = k)$\n",
    "- Component probabilities: $\\theta_{rkc} = P(X^{(r)} = c | H = k)$\n",
    "## Identifiability\n",
    "\n",
    "To resolve label switching and identifiability issues, we impose the constraint:\n",
    "\n",
    "$$\\pi_1 \\geq \\pi_2 \\geq \\cdots \\geq \\pi_K$$\n",
    "\n",
    "After fitting the model, latent classes are automatically sorted by their mixture weights in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Generation\n",
    "\n",
    "Generate synthetic data from a true latent class model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentClassDataGenerator:\n",
    "    \"\"\"Generate synthetic data from a latent class model.\"\"\"\n",
    "    \n",
    "    def __init__(self, m: int, K: int, C_r: int, seed: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        m : int\n",
    "            Number of categorical variables\n",
    "        K : int\n",
    "            Number of latent classes\n",
    "        C_r : int\n",
    "            Number of categories for each variable (assumed same for all)\n",
    "        seed : int, optional\n",
    "            Random seed\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        self.m = m\n",
    "        self.K = K\n",
    "        self.C_r = C_r\n",
    "        \n",
    "        # Generate true parameters\n",
    "        self.true_pi = self._generate_mixture_weights()\n",
    "        self.true_theta = self._generate_component_probs()\n",
    "        \n",
    "        # Sort classes by mixture weights (pi_1 >= pi_2 >= ... >= pi_K)\n",
    "        self._sort_classes()\n",
    "    \n",
    "    def _generate_mixture_weights(self) -> np.ndarray:\n",
    "        \"\"\"Generate mixture weights that sum to 1.\"\"\"\n",
    "        pi = np.random.dirichlet(np.ones(self.K))\n",
    "        return pi\n",
    "    \n",
    "    def _generate_component_probs(self) -> np.ndarray:\n",
    "        \"\"\"Generate component categorical probabilities.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        theta : np.ndarray of shape (K, m, C_r)\n",
    "            theta[k, r, c] = P(X^(r) = c | H = k)\n",
    "        \"\"\"\n",
    "        theta = np.zeros((self.K, self.m, self.C_r))\n",
    "        \n",
    "        for k in range(self.K):\n",
    "            for r in range(self.m):\n",
    "                # Sample from Dirichlet to ensure probabilities sum to 1\n",
    "                theta[k, r, :] = np.random.dirichlet(np.ones(self.C_r))\n",
    "        \n",
    "        return theta\n",
    "    \n",
    "    def _sort_classes(self):\n",
    "        \"\"\"Sort latent classes by mixture weights in descending order.\"\"\"\n",
    "        sort_idx = np.argsort(self.true_pi)[::-1]\n",
    "        self.true_pi = self.true_pi[sort_idx]\n",
    "        self.true_theta = self.true_theta[sort_idx, :, :]\n",
    "    \n",
    "    def generate_data(self, n: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Generate n samples from the latent class model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n : int\n",
    "            Number of samples to generate\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X : np.ndarray of shape (n, m)\n",
    "            Observed categorical data (values in {0, 1, ..., C_r-1})\n",
    "        H : np.ndarray of shape (n,)\n",
    "            True latent class assignments (values in {0, 1, ..., K-1})\n",
    "        \"\"\"\n",
    "        # Sample latent classes\n",
    "        H = np.random.choice(self.K, size=n, p=self.true_pi)\n",
    "        \n",
    "        # Sample observations given latent classes\n",
    "        X = np.zeros((n, self.m), dtype=int)\n",
    "        \n",
    "        for i in range(n):\n",
    "            k = H[i]\n",
    "            for r in range(self.m):\n",
    "                # Sample X^(r) from categorical distribution given H=k\n",
    "                X[i, r] = np.random.choice(self.C_r, p=self.true_theta[k, r, :])\n",
    "        \n",
    "        return X, H\n",
    "    \n",
    "    def get_true_parameters(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Return true parameters for comparison.\"\"\"\n",
    "        return {\n",
    "            'pi': self.true_pi.copy(),\n",
    "            'theta': self.true_theta.copy()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EM Algorithm Implementation\n",
    "\n",
    "Implement the EM algorithm with numerically stable log-space calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentClassEM:\n",
    "    \"\"\"EM algorithm for latent class model estimation.\"\"\"\n",
    "    \n",
    "    def __init__(self, m: int, K: int, C_r: int, seed: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        m : int\n",
    "            Number of categorical variables\n",
    "        K : int\n",
    "            Number of latent classes\n",
    "        C_r : int\n",
    "            Number of categories for each variable\n",
    "        seed : int, optional\n",
    "            Random seed for initialization\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.K = K\n",
    "        self.C_r = C_r\n",
    "        self.seed = seed\n",
    "        \n",
    "        # Parameters to be estimated\n",
    "        self.pi = None  # Shape: (K,)\n",
    "        self.theta = None  # Shape: (K, m, C_r)\n",
    "        \n",
    "        # Training history\n",
    "        self.log_likelihoods = []\n",
    "        self.n_iterations = 0\n",
    "    \n",
    "    def _initialize_parameters(self):\n",
    "        \"\"\"Randomly initialize parameters.\"\"\"\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        \n",
    "        # Initialize mixture weights\n",
    "        self.pi = np.random.dirichlet(np.ones(self.K))\n",
    "        \n",
    "        # Initialize component probabilities\n",
    "        self.theta = np.zeros((self.K, self.m, self.C_r))\n",
    "        for k in range(self.K):\n",
    "            for r in range(self.m):\n",
    "                self.theta[k, r, :] = np.random.dirichlet(np.ones(self.C_r))\n",
    "    \n",
    "    def _compute_log_likelihood(self, X: np.ndarray) -> float:\n",
    "        \"\"\"Compute log-likelihood of the data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray of shape (n, m)\n",
    "            Observed data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        log_lik : float\n",
    "            Log-likelihood value\n",
    "        \"\"\"\n",
    "        n = X.shape[0]\n",
    "        log_lik = 0.0\n",
    "        \n",
    "        # Vectorized computation\n",
    "        # For each sample i and class k, compute log P(X_i | H=k)\n",
    "        log_class_probs = np.zeros((n, self.K))\n",
    "        \n",
    "        for k in range(self.K):\n",
    "            # log P(H=k)\n",
    "            log_class_probs[:, k] = np.log(self.pi[k])\n",
    "            \n",
    "            # Add log P(X^(r) | H=k) for each r\n",
    "            for r in range(self.m):\n",
    "                # Extract the probabilities for observed categories\n",
    "                log_class_probs[:, k] += np.log(self.theta[k, r, X[:, r]])\n",
    "        \n",
    "        # Use log-sum-exp trick for numerical stability\n",
    "        max_log_prob = np.max(log_class_probs, axis=1, keepdims=True)\n",
    "        log_lik = np.sum(max_log_prob + np.log(np.sum(np.exp(log_class_probs - max_log_prob), axis=1, keepdims=True)))\n",
    "        \n",
    "        return log_lik\n",
    "    \n",
    "    def _e_step(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"E-step: Compute posterior probabilities gamma_ik.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray of shape (n, m)\n",
    "            Observed data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        gamma : np.ndarray of shape (n, K)\n",
    "            Posterior probabilities gamma[i, k] = P(H_i = k | X_i)\n",
    "        \"\"\"\n",
    "        n = X.shape[0]\n",
    "        \n",
    "        # Compute a_ik = log pi_k + sum_r log theta_{rk,X_i^(r)}\n",
    "        a = np.zeros((n, self.K))\n",
    "        \n",
    "        for k in range(self.K):\n",
    "            a[:, k] = np.log(self.pi[k])\n",
    "            for r in range(self.m):\n",
    "                a[:, k] += np.log(self.theta[k, r, X[:, r]])\n",
    "        \n",
    "        # Numerically stable computation of gamma using log-space\n",
    "        # gamma_ik = exp(a_ik - M_i) / sum_j exp(a_ij - M_i)\n",
    "        M = np.max(a, axis=1, keepdims=True)\n",
    "        exp_a = np.exp(a - M)\n",
    "        gamma = exp_a / np.sum(exp_a, axis=1, keepdims=True)\n",
    "        \n",
    "        return gamma\n",
    "    \n",
    "    def _m_step(self, X: np.ndarray, gamma: np.ndarray):\n",
    "        \"\"\"M-step: Update parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray of shape (n, m)\n",
    "            Observed data\n",
    "        gamma : np.ndarray of shape (n, K)\n",
    "            Posterior probabilities from E-step\n",
    "        \"\"\"\n",
    "        n = X.shape[0]\n",
    "        \n",
    "        # Update mixture weights: pi_k = (1/n) * sum_i gamma_ik\n",
    "        self.pi = np.mean(gamma, axis=0)\n",
    "        \n",
    "        # Update component probabilities: theta_{rkc} = sum_i gamma_ik * 1(X_i^(r) = c) / sum_i gamma_ik\n",
    "        for k in range(self.K):\n",
    "            for r in range(self.m):\n",
    "                for c in range(self.C_r):\n",
    "                    # Vectorized computation\n",
    "                    mask = (X[:, r] == c).astype(float)\n",
    "                    self.theta[k, r, c] = np.sum(gamma[:, k] * mask) / np.sum(gamma[:, k])\n",
    "        \n",
    "        # Ensure numerical stability (avoid zeros)\n",
    "        self.theta = np.maximum(self.theta, 1e-10)\n",
    "        # Renormalize to ensure probabilities sum to 1\n",
    "        self.theta = self.theta / np.sum(self.theta, axis=2, keepdims=True)\n",
    "    \n",
    "    def fit(self, X: np.ndarray, max_iter: int = 100, tol: float = 1e-6, verbose: bool = True) -> Dict:\n",
    "        \"\"\"Fit the latent class model using EM algorithm.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray of shape (n, m)\n",
    "            Observed data\n",
    "        max_iter : int\n",
    "            Maximum number of iterations\n",
    "        tol : float\n",
    "            Convergence tolerance for log-likelihood change\n",
    "        verbose : bool\n",
    "            Whether to print progress\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        history : dict\n",
    "            Training history including log-likelihoods\n",
    "        \"\"\"\n",
    "        # Initialize parameters\n",
    "        self._initialize_parameters()\n",
    "        \n",
    "        # Initialize history\n",
    "        self.log_likelihoods = []\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Starting EM algorithm...\")\n",
    "            print(f\"Data: n={X.shape[0]}, m={self.m}, K={self.K}, C_r={self.C_r}\")\n",
    "        \n",
    "        for iteration in range(max_iter):\n",
    "            # E-step\n",
    "            gamma = self._e_step(X)\n",
    "            \n",
    "            # M-step\n",
    "            self._m_step(X, gamma)\n",
    "            \n",
    "            # Compute log-likelihood\n",
    "            log_lik = self._compute_log_likelihood(X)\n",
    "            self.log_likelihoods.append(log_lik)\n",
    "            \n",
    "            if verbose and (iteration + 1) % 10 == 0:\n",
    "                print(f\"Iteration {iteration + 1}/{max_iter}, Log-likelihood: {log_lik:.4f}\")\n",
    "            \n",
    "            # Check convergence\n",
    "            if iteration > 0:\n",
    "                log_lik_change = abs(self.log_likelihoods[-1] - self.log_likelihoods[-2])\n",
    "                if log_lik_change < tol:\n",
    "                    if verbose:\n",
    "                        print(f\"\\nConverged at iteration {iteration + 1}\")\n",
    "                        print(f\"Log-likelihood change: {log_lik_change:.6e} < {tol}\")\n",
    "                    self.n_iterations = iteration + 1\n",
    "                    break\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"\\nReached maximum iterations: {max_iter}\")\n",
    "            self.n_iterations = max_iter\n",
    "        \n",
    "        \n",
    "        # Sort classes by mixture weights (resolve identifiability)\n",
    "        self._sort_classes()\n",
    "        if verbose:\n",
    "            print(f\"Final log-likelihood: {self.log_likelihoods[-1]:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'log_likelihoods': self.log_likelihoods,\n",
    "            'n_iterations': self.n_iterations,\n",
    "            'final_log_lik': self.log_likelihoods[-1]\n",
    "        }\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict posterior class probabilities for new data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray of shape (n, m)\n",
    "            Observed data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        gamma : np.ndarray of shape (n, K)\n",
    "            Posterior class probabilities\n",
    "        \"\"\"\n",
    "        return self._e_step(X)\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict class assignments for new data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray of shape (n, m)\n",
    "            Observed data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        H_pred : np.ndarray of shape (n,)\n",
    "            Predicted class assignments\n",
    "        \"\"\"\n",
    "        gamma = self.predict_proba(X)\n",
    "        return np.argmax(gamma, axis=1)\n",
    "    \n",
    "    def _sort_classes(self):\n",
    "        \"\"\"Sort latent classes by mixture weights in descending order.\n",
    "        \n",
    "        This resolves identifiability issues by imposing the constraint:\n",
    "        pi_1 >= pi_2 >= ... >= pi_K\n",
    "        \"\"\"\n",
    "        # Get sorting indices (descending order)\n",
    "        sort_idx = np.argsort(self.pi)[::-1]\n",
    "        \n",
    "        # Reorder mixture weights\n",
    "        self.pi = self.pi[sort_idx]\n",
    "        \n",
    "        # Reorder component probabilities\n",
    "        self.theta = self.theta[sort_idx, :, :]\n",
    "    \n",
    "    def get_parameters(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Return estimated parameters.\"\"\"\n",
    "        return {\n",
    "            'pi': self.pi.copy(),\n",
    "            'theta': self.theta.copy()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence(log_likelihoods: list, title: str = \"EM Algorithm Convergence\"):\n",
    "    \"\"\"Plot log-likelihood convergence.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(log_likelihoods) + 1), log_likelihoods, 'b-', linewidth=2)\n",
    "    plt.xlabel('Iteration', fontsize=12)\n",
    "    plt.ylabel('Log-Likelihood', fontsize=12)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_parameter_comparison(true_params: Dict, estimated_params: Dict, param_name: str = 'pi'):\n",
    "    \"\"\"Compare true and estimated parameters.\"\"\"\n",
    "    if param_name == 'pi':\n",
    "        true_pi = true_params['pi']\n",
    "        est_pi = estimated_params['pi']\n",
    "        K = len(true_pi)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        x = np.arange(K)\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, true_pi, width, label='True', alpha=0.8)\n",
    "        plt.bar(x + width/2, est_pi, width, label='Estimated', alpha=0.8)\n",
    "        \n",
    "        plt.xlabel('Class k', fontsize=12)\n",
    "        plt.ylabel('Mixture Weight π_k', fontsize=12)\n",
    "        plt.title('Comparison of Mixture Weights', fontsize=14, fontweight='bold')\n",
    "        plt.xticks(x, [f'Class {k}' for k in range(K)])\n",
    "        plt.legend(fontsize=11)\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print numerical comparison\n",
    "        print(\"\\nMixture Weights Comparison:\")\n",
    "        print(f\"{'Class':<10} {'True':<15} {'Estimated':<15} {'Absolute Error':<15}\")\n",
    "        print(\"-\" * 60)\n",
    "        for k in range(K):\n",
    "            error = abs(true_pi[k] - est_pi[k])\n",
    "            print(f\"{k:<10} {true_pi[k]:<15.4f} {est_pi[k]:<15.4f} {error:<15.4f}\")\n",
    "        print(f\"\\nMean Absolute Error: {np.mean(np.abs(true_pi - est_pi)):.6f}\")\n",
    "\n",
    "\n",
    "def plot_theta_comparison(true_params: Dict, estimated_params: Dict, k: int = 0, r: int = 0):\n",
    "    \"\"\"Compare true and estimated theta for a specific class k and variable r.\"\"\"\n",
    "    true_theta = true_params['theta'][k, r, :]\n",
    "    est_theta = estimated_params['theta'][k, r, :]\n",
    "    C_r = len(true_theta)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x = np.arange(C_r)\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, true_theta, width, label='True', alpha=0.8)\n",
    "    plt.bar(x + width/2, est_theta, width, label='Estimated', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Category c', fontsize=12)\n",
    "    plt.ylabel(f'θ_{{{r}{k}c}}', fontsize=12)\n",
    "    plt.title(f'Comparison of Component Probabilities (Class k={k}, Variable r={r})', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xticks(x, [f'Cat {c}' for c in range(C_r)])\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_theta_heatmap(params: Dict, title: str = \"Component Probabilities θ\"):\n",
    "    \"\"\"Plot heatmap of theta parameters.\n",
    "    \n",
    "    For each class k, shows m x C_r heatmap of probabilities.\n",
    "    \"\"\"\n",
    "    theta = params['theta']  # Shape: (K, m, C_r)\n",
    "    K, m, C_r = theta.shape\n",
    "    \n",
    "    fig, axes = plt.subplots(1, K, figsize=(5*K, 6))\n",
    "    if K == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for k in range(K):\n",
    "        sns.heatmap(theta[k, :, :], annot=True, fmt='.3f', cmap='YlOrRd', \n",
    "                    cbar=True, ax=axes[k], vmin=0, vmax=1)\n",
    "        axes[k].set_xlabel('Category c', fontsize=11)\n",
    "        axes[k].set_ylabel('Variable r', fontsize=11)\n",
    "        axes[k].set_title(f'Class k={k}', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    fig.suptitle(title, fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_class_assignments(true_classes: np.ndarray, predicted_classes: np.ndarray):\n",
    "    \"\"\"Visualize true vs predicted class assignments.\"\"\"\n",
    "    # Confusion matrix\n",
    "    K = len(np.unique(true_classes))\n",
    "    confusion = np.zeros((K, K))\n",
    "    \n",
    "    for true_k in range(K):\n",
    "        for pred_k in range(K):\n",
    "            confusion[true_k, pred_k] = np.sum((true_classes == true_k) & (predicted_classes == pred_k))\n",
    "    \n",
    "    plt.figure(figsize=(8, 7))\n",
    "    sns.heatmap(confusion, annot=True, fmt='.0f', cmap='Blues', cbar=True)\n",
    "    plt.xlabel('Predicted Class', fontsize=12)\n",
    "    plt.ylabel('True Class', fontsize=12)\n",
    "    plt.title('Confusion Matrix: True vs Predicted Class Assignments', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate accuracy (accounting for label permutation)\n",
    "    from scipy.optimize import linear_sum_assignment\n",
    "    row_ind, col_ind = linear_sum_assignment(-confusion)\n",
    "    accuracy = confusion[row_ind, col_ind].sum() / len(true_classes)\n",
    "    \n",
    "    print(f\"\\nClassification Accuracy (best label matching): {accuracy:.4f}\")\n",
    "    print(f\"Optimal label mapping: {dict(zip(row_ind, col_ind))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Experiment\n",
    "\n",
    "Run the complete experiment with data generation, model fitting, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PARAMETERS - Easily adjustable\n",
    "# ============================================================\n",
    "\n",
    "# Data parameters\n",
    "m = 40          # Number of categorical variables\n",
    "K = 4           # Number of latent classes\n",
    "C_r = 3         # Number of categories per variable\n",
    "n = 2000        # Number of samples\n",
    "\n",
    "# EM parameters\n",
    "max_iter = 200  # Maximum EM iterations\n",
    "tol = 1e-6      # Convergence tolerance\n",
    "\n",
    "# Random seeds\n",
    "data_seed = 42\n",
    "em_seed = 123\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LATENT CLASS MODEL EXPERIMENT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nData Parameters:\")\n",
    "print(f\"  - Number of variables (m): {m}\")\n",
    "print(f\"  - Number of latent classes (K): {K}\")\n",
    "print(f\"  - Categories per variable (C_r): {C_r}\")\n",
    "print(f\"  - Number of samples (n): {n}\")\n",
    "print(f\"\\nEM Parameters:\")\n",
    "print(f\"  - Maximum iterations: {max_iter}\")\n",
    "print(f\"  - Convergence tolerance: {tol}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 1: Generate Synthetic Data\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n[STEP 1] Generating synthetic data...\")\n",
    "\n",
    "generator = LatentClassDataGenerator(m=m, K=K, C_r=C_r, seed=data_seed)\n",
    "X, H_true = generator.generate_data(n=n)\n",
    "true_params = generator.get_true_parameters()\n",
    "\n",
    "print(f\"✓ Generated {n} samples\")\n",
    "print(f\"  Data shape: {X.shape}\")\n",
    "print(f\"  True class distribution: {np.bincount(H_true)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 2: Fit Latent Class Model using EM\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n[STEP 2] Fitting latent class model...\\n\")\n",
    "\n",
    "model = LatentClassEM(m=m, K=K, C_r=C_r, seed=em_seed)\n",
    "history = model.fit(X, max_iter=max_iter, tol=tol, verbose=True)\n",
    "\n",
    "estimated_params = model.get_parameters()\n",
    "\n",
    "print(\"\\n✓ Model fitting complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 3: Make Predictions\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n[STEP 3] Making predictions...\")\n",
    "\n",
    "H_pred = model.predict(X)\n",
    "gamma_pred = model.predict_proba(X)\n",
    "\n",
    "print(f\"✓ Predictions complete\")\n",
    "print(f\"  Predicted class distribution: {np.bincount(H_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizations and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION 1: Convergence Plot\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n[VISUALIZATION] Plotting convergence...\\n\")\n",
    "plot_convergence(history['log_likelihoods'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION 2: Mixture Weights Comparison\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n[VISUALIZATION] Comparing mixture weights...\\n\")\n",
    "plot_parameter_comparison(true_params, estimated_params, param_name='pi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION 3: Component Probabilities - Sample Comparison\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n[VISUALIZATION] Comparing component probabilities (sample)...\\n\")\n",
    "\n",
    "# Show comparison for class 0, variable 0\n",
    "plot_theta_comparison(true_params, estimated_params, k=0, r=0)\n",
    "\n",
    "# Show comparison for another example\n",
    "if K > 1 and m > 1:\n",
    "    plot_theta_comparison(true_params, estimated_params, k=1, r=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION 4: Theta Heatmaps\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n[VISUALIZATION] Plotting theta heatmaps...\\n\")\n",
    "\n",
    "plot_theta_heatmap(true_params, title=\"True Component Probabilities θ\")\n",
    "plot_theta_heatmap(estimated_params, title=\"Estimated Component Probabilities θ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION 5: Class Assignments\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n[VISUALIZATION] Comparing class assignments...\\n\")\n",
    "plot_class_assignments(H_true, H_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# QUANTITATIVE EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUANTITATIVE EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nNote: Both true and estimated classes are sorted by mixture weights\")\n",
    "print(\"      (π_1 >= π_2 >= ... >= π_K) to resolve identifiability.\\n\")\n",
    "\n",
    "# Mixture weights error\n",
    "pi_mae = np.mean(np.abs(true_params['pi'] - estimated_params['pi']))\n",
    "pi_rmse = np.sqrt(np.mean((true_params['pi'] - estimated_params['pi'])**2))\n",
    "\n",
    "print(f\"\\nMixture Weights:\")\n",
    "print(f\"  Mean Absolute Error (MAE): {pi_mae:.6f}\")\n",
    "print(f\"  Root Mean Squared Error (RMSE): {pi_rmse:.6f}\")\n",
    "\n",
    "# Component probabilities error\n",
    "theta_mae = np.mean(np.abs(true_params['theta'] - estimated_params['theta']))\n",
    "theta_rmse = np.sqrt(np.mean((true_params['theta'] - estimated_params['theta'])**2))\n",
    "\n",
    "print(f\"\\nComponent Probabilities:\")\n",
    "print(f\"  Mean Absolute Error (MAE): {theta_mae:.6f}\")\n",
    "print(f\"  Root Mean Squared Error (RMSE): {theta_rmse:.6f}\")\n",
    "\n",
    "# Convergence statistics\n",
    "print(f\"\\nConvergence:\")\n",
    "print(f\"  Number of iterations: {history['n_iterations']}\")\n",
    "print(f\"  Final log-likelihood: {history['final_log_lik']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Additional Analysis (Optional)\n",
    "\n",
    "Explore posterior probabilities and uncertainty in class assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Posterior Probability Analysis\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nPosterior Probability Statistics:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# For each true class, show average posterior probability assigned to each class\n",
    "for k_true in range(K):\n",
    "    mask = (H_true == k_true)\n",
    "    avg_posteriors = np.mean(gamma_pred[mask, :], axis=0)\n",
    "    print(f\"\\nTrue Class {k_true}:\")\n",
    "    for k_pred in range(K):\n",
    "        print(f\"  Avg P(H={k_pred} | X): {avg_posteriors[k_pred]:.4f}\")\n",
    "\n",
    "# Show samples with highest uncertainty\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Most Uncertain Predictions (entropy):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compute entropy of posterior probabilities\n",
    "entropy = -np.sum(gamma_pred * np.log(gamma_pred + 1e-10), axis=1)\n",
    "most_uncertain_idx = np.argsort(entropy)[-5:][::-1]\n",
    "\n",
    "print(f\"\\n{'Sample':<10} {'True Class':<12} {'Pred Class':<12} {'Entropy':<12} Posterior Probs\")\n",
    "print(\"-\" * 80)\n",
    "for idx in most_uncertain_idx:\n",
    "    probs_str = \"  \".join([f\"{p:.3f}\" for p in gamma_pred[idx]])\n",
    "    print(f\"{idx:<10} {H_true[idx]:<12} {H_pred[idx]:<12} {entropy[idx]:<12.4f} [{probs_str}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "The notebook implements a complete pipeline for:\n",
    "1. Generating synthetic data from a latent class model\n",
    "2. Estimating model parameters using the EM algorithm with numerically stable computations\n",
    "3. Visualizing convergence, parameter recovery, and class assignments\n",
    "4. Quantitative evaluation of estimation accuracy\n",
    "\n",
    "Key features:\n",
    "- Vectorized operations for computational efficiency\n",
    "- Log-space calculations to avoid numerical underflow\n",
    "- Dual stopping criteria (convergence + max iterations)\n",
    "- Comprehensive visualizations\n",
    "- Easy parameter control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
